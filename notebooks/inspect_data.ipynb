{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc90e5c0",
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "source": [
    "**Stage**: Inspect Raw Data\n",
    "\n",
    "**Objectives**:\n",
    "- Verify raw stock data integrity (row count, date range, data types).\n",
    "- Identify gaps in trading days, missing values, and anomalies.\n",
    "- Visualize closing price and trading volume to detect trends or outliers.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c337c2fa",
   "metadata": {},
   "source": [
    "## Data Source and Preparation\n",
    "Raw stock data is sourced from a financial data provider and prepared for analysis. The dataset is dynamically selected based on the specified stock symbol, ensuring adaptability across different stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ec2d3a",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters for automation\n",
    "stock_symbol = None\n",
    "fetch_id = None\n",
    "config_path = \"config/config.json\"\n",
    "auto_confirm_outliers = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90790c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import exchange_calendars as ec\n",
    "from typing import Dict, List, Optional, Callable\n",
    "from spp.logging_utils import setup_logging\n",
    "from spp.data_utils import parse_arguments, get_stock_symbol, get_fetch_id, load_config, load_data, format_df\n",
    "from spp.plot_utils import plot_time_series\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Define project root directory\n",
    "config = load_config(config_path=\"config/config.json\", logger=setup_logging(logger_name=\"inspecting_data\", log_dir=\"logs\"))\n",
    "PROJECT_ROOT = config[\"project_root\"]\n",
    "\n",
    "# Initialize logger\n",
    "logger = setup_logging(logger_name=\"inspecting_data\", log_dir=\"logs\")\n",
    "\n",
    "\n",
    "def verify_gaps(\n",
    "    df: pd.DataFrame,\n",
    "    fetch_id: str,\n",
    "    exchange: str = 'XNAS',\n",
    "    start_date: Optional[str] = None,\n",
    "    end_date: Optional[str] = None\n",
    ") -> Dict:\n",
    "    \"\"\"Verify gaps in stock data using the exchange calendar.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Stock data DataFrame with a DatetimeIndex.\n",
    "        fetch_id (str): Unique identifier for the data fetch.\n",
    "        exchange (str, optional): Exchange calendar name (e.g., 'XNAS' for NASDAQ). Defaults to 'XNAS'.\n",
    "        start_date (str or datetime, optional): Start date for the analysis.\n",
    "        end_date (str or datetime, optional): End date for the analysis.\n",
    "    \n",
    "    Returns:\n",
    "        Dict: A dictionary containing gap verification results.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If the DataFrame index is not a DatetimeIndex.\n",
    "        KeyError: If the specified exchange calendar is not found.\n",
    "    \"\"\"\n",
    "    # Validate that the DataFrame index is a DatetimeIndex\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        logger.error(\"DataFrame index must be a DatetimeIndex\", extra={\"fetch_id\": fetch_id})\n",
    "        raise ValueError(\"DataFrame index must be a DatetimeIndex\")\n",
    "    \n",
    "    # Ensure the DataFrame is sorted by index\n",
    "    df = df.sort_index()\n",
    "    \n",
    "    # Convert start_date and end_date to timezone-naive timestamps\n",
    "    start_date = pd.to_datetime(start_date or df.index.min()).tz_localize(None)\n",
    "    end_date = pd.to_datetime(end_date or df.index.max()).tz_localize(None)\n",
    "\n",
    "    # Get the exchange calendar\n",
    "    try:\n",
    "        cal = ec.get_calendar(exchange)\n",
    "    except KeyError as e:\n",
    "        logger.error(f\"Invalid exchange calendar: {exchange}\", extra={\"fetch_id\": fetch_id})\n",
    "        raise KeyError(f\"Invalid exchange calendar: {exchange}\") from e\n",
    "\n",
    "    # Get trading days\n",
    "    trading_days_naive = cal.sessions_in_range(start_date, end_date)\n",
    "    trading_days = trading_days_naive.tz_localize(cal.tz).tz_convert('UTC')\n",
    "    \n",
    "    # Convert both to date-only for comparison\n",
    "    trading_days_dates = trading_days.date  # Array of datetime.date objects\n",
    "    df_dates = df.index.date  # Array of datetime.date objects\n",
    "\n",
    "    # Analyze gaps and other metrics\n",
    "    result = {}\n",
    "    \n",
    "    # Check for weekend dates\n",
    "    is_weekend = df.index.weekday >= 5\n",
    "    result['weekend_dates'] = [d.isoformat() for d in df_dates[is_weekend]]\n",
    "    \n",
    "    # Calculate gaps\n",
    "    date_series = pd.Series(df_dates, index=df.index)\n",
    "    date_diff = date_series.diff().apply(lambda x: x.days if pd.notnull(x) else 0)\n",
    "    gaps = date_diff[date_diff > 1].index\n",
    "    result['gaps'] = [{'after': df_dates[df.index.get_loc(g) - 1].isoformat(), 'days': int(date_diff[g])} for g in gaps]\n",
    "    \n",
    "    # Missing trading days and non-trading index days using set operations\n",
    "    trading_days_set = set(trading_days_dates)\n",
    "    df_dates_set = set(df_dates)\n",
    "    result['missing_trading_days'] = [d.isoformat() for d in trading_days_set - df_dates_set]\n",
    "    result['non_trading_index_days'] = [d.isoformat() for d in df_dates_set - trading_days_set]\n",
    "    \n",
    "    # Logging\n",
    "    if result['weekend_dates']:\n",
    "        logger.warning(f\"Weekend dates found: {len(result['weekend_dates'])} dates\", extra={\"fetch_id\": fetch_id})\n",
    "        logger.debug(f\"Weekend dates details: {result['weekend_dates']}\", extra={\"fetch_id\": fetch_id})\n",
    "    else:\n",
    "        logger.info(\"No weekend dates in index\", extra={\"fetch_id\": fetch_id})\n",
    "\n",
    "    if result['gaps']:\n",
    "        logger.warning(f\"Dates with gaps detected: {len(result['gaps'])} gaps\", extra={\"fetch_id\": fetch_id})\n",
    "        logger.debug(\"Gap details:\", extra={\"fetch_id\": fetch_id})\n",
    "        for gap in result['gaps']:\n",
    "            logger.debug(f\"Gap after {gap['after']}: {gap['days']} days\", extra={\"fetch_id\": fetch_id})\n",
    "    else:\n",
    "        logger.info(\"No gaps in dates\", extra={\"fetch_id\": fetch_id})\n",
    "\n",
    "    if result['missing_trading_days']:\n",
    "        logger.warning(f\"Trading days missing in index: {len(result['missing_trading_days'])} days\", extra={\"fetch_id\": fetch_id})\n",
    "        logger.debug(f\"Missing trading days: {result['missing_trading_days']}\", extra={\"fetch_id\": fetch_id})\n",
    "    else:\n",
    "        logger.info(\"No trading days missing in index\", extra={\"fetch_id\": fetch_id})\n",
    "\n",
    "    if result['non_trading_index_days']:\n",
    "        logger.warning(f\"Non-trading days found in index: {len(result['non_trading_index_days'])} days\", extra={\"fetch_id\": fetch_id})\n",
    "        logger.debug(f\"Non-trading days details: {result['non_trading_index_days']}\", extra={\"fetch_id\": fetch_id})\n",
    "    else:\n",
    "        logger.info(\"All index days are trading days\", extra={\"fetch_id\": fetch_id})\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def export_gaps(\n",
    "        gaps: Dict, \n",
    "        stock_symbol: str, \n",
    "        fetch_id: str\n",
    ") -> None:\n",
    "    \"\"\"Save identified gaps to JSON files for active and historical records.\n",
    "    \n",
    "    Args:\n",
    "        gaps (Dict): Dictionary containing gap verification results.\n",
    "        stock_symbol (str): Stock symbol for the data.\n",
    "        fetch_id (str): Unique identifier for the data fetch.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \n",
    "    Raises:\n",
    "        OSError: If there are issues creating directories or writing to files.\n",
    "        TypeError: If the data cannot be serialized to JSON.\n",
    "    \"\"\"\n",
    "    data = {\n",
    "        \"stock_symbol\": stock_symbol,\n",
    "        \"fetch_id\": fetch_id,\n",
    "        \"gaps\": gaps\n",
    "    }\n",
    "    gaps_dir = os.path.join(PROJECT_ROOT, \"data\", \"intermediate\")\n",
    "    os.makedirs(gaps_dir, exist_ok=True)\n",
    "    active_file = os.path.join(gaps_dir, \"gaps.json\")\n",
    "    history_file = os.path.join(gaps_dir, \"gaps_history.jsonl\")\n",
    "    \n",
    "    try:\n",
    "        with open(active_file, 'w') as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "        with open(history_file, 'a') as f:\n",
    "            f.write(json.dumps(data) + '\\n')\n",
    "        logger.info(f\"Gap results exported to {active_file} and appended to {history_file}\", extra={\"fetch_id\": fetch_id})\n",
    "    except OSError as e:\n",
    "        logger.error(f\"Failed to write gap results: {str(e)}\", extra={\"fetch_id\": fetch_id})\n",
    "        raise\n",
    "    except TypeError as e:\n",
    "        logger.error(f\"Invalid data format for JSON serialization: {str(e)}\", extra={\"fetch_id\": fetch_id})\n",
    "        raise\n",
    "\n",
    "\n",
    "def verify_data(\n",
    "        df: pd.DataFrame, \n",
    "        fetch_id: str\n",
    ") -> None:\n",
    "    \"\"\"Verify the integrity of stock data, logging key metrics and anomalies.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Stock data DataFrame to verify.\n",
    "        fetch_id (str): Unique identifier for the data fetch.\n",
    "    \"\"\"\n",
    "    row_count = len(df)\n",
    "    logger.info(f\"Row Count: {row_count}\", extra={\"fetch_id\": fetch_id})\n",
    "\n",
    "    start_date, end_date = df.index.min(), df.index.max()\n",
    "    date_range_days = (end_date - start_date).days + 1\n",
    "    logger.info(f\"Date Range: {start_date.date()} to {end_date.date()} ({date_range_days} days)\", extra={\"fetch_id\": fetch_id})\n",
    "\n",
    "    missing_values = df.isna().sum()\n",
    "    if missing_values.any():\n",
    "        logger.warning(f\"Missing values found: {missing_values[missing_values > 0].to_dict()}\", extra={\"fetch_id\": fetch_id})\n",
    "    else:\n",
    "        logger.info(\"No missing values found\", extra={\"fetch_id\": fetch_id})\n",
    "\n",
    "    negative_prices = df[['open', 'high', 'low', 'close']].lt(0).sum()\n",
    "    if negative_prices.any():\n",
    "        logger.warning(f\"Negative prices found: {negative_prices[negative_prices > 0].to_dict()}\", extra={\"fetch_id\": fetch_id})\n",
    "    negative_volume = (df['volume'] < 0).sum()\n",
    "    if negative_volume > 0:\n",
    "        logger.warning(f\"Negative volumes: {negative_volume}\", extra={\"fetch_id\": fetch_id})\n",
    "\n",
    "    logger.debug(f\"Data Types:\\n{df.dtypes.to_string()}\", extra={\"fetch_id\": fetch_id})\n",
    "    logger.debug(f\"Statistics Summary:\\n{df.describe().to_string()}\", extra={\"fetch_id\": fetch_id})\n",
    "\n",
    "    if not (missing_values.any() or negative_prices.any() or negative_volume > 0):\n",
    "        logger.info(\"Data verification completed successfully\", extra={\"fetch_id\": fetch_id})\n",
    "\n",
    "\n",
    "def detect_anomalies(\n",
    "    df: pd.DataFrame, \n",
    "    fetch_id: str, \n",
    "    detection_method: Callable[[pd.Series, bool], pd.Series]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Detect anomalies in stock data using a specified detection method for each column.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Stock data DataFrame with expected columns.\n",
    "        fetch_id (str): Unique identifier for the data fetch.\n",
    "        detection_method (Callable): A function that takes a pandas Series and a boolean \n",
    "            (indicating whether to apply log transformation) and returns a boolean Series \n",
    "            indicating anomalies.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with columns 'date', 'column', 'value' for anomalies.\n",
    "    \n",
    "    Raises:\n",
    "        TypeError: If df is not a pandas DataFrame.\n",
    "        ValueError: If df is empty or missing required columns.\n",
    "    \"\"\"\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        logger.error(f\"Expected pandas DataFrame, got {type(df)}\", extra={\"fetch_id\": fetch_id})\n",
    "        raise TypeError(f\"Expected pandas DataFrame, got {type(df)}\")\n",
    "    \n",
    "    if df.empty:\n",
    "        logger.error(\"Input DataFrame is empty\", extra={\"fetch_id\": fetch_id})\n",
    "        raise ValueError(\"Input DataFrame is empty\")\n",
    "    \n",
    "    required_columns = ['open', 'high', 'low', 'close', 'volume']\n",
    "    missing_cols = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        logger.error(f\"Missing required columns: {missing_cols}\", extra={\"fetch_id\": fetch_id})\n",
    "        raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "    \n",
    "    logger.debug(f\"Processing columns: {list(df.columns)}\", extra={\"fetch_id\": fetch_id})\n",
    "    \n",
    "    columns_log = ['volume']\n",
    "    anomalies_list = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            is_anomaly = detection_method(df[col], log_transform=(col in columns_log))\n",
    "            if is_anomaly.any():\n",
    "                for date in df.index[is_anomaly]:\n",
    "                    anomalies_list.append({\n",
    "                        'date': date.strftime('%Y-%m-%d'),\n",
    "                        'column': col,\n",
    "                        'value': df.at[date, col]\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing column {col}: {str(e)}\", extra={\"fetch_id\": fetch_id})\n",
    "            raise\n",
    "    \n",
    "    anomalies_df = pd.DataFrame(anomalies_list, columns=['date', 'column', 'value'], \n",
    "                               index=range(1, len(anomalies_list) + 1))\n",
    "    return anomalies_df\n",
    "\n",
    "\n",
    "def detect_anomalies_iqr_series(\n",
    "        series: pd.Series, \n",
    "        log_transform: bool = False\n",
    ") -> pd.Series:\n",
    "    \"\"\"Detect anomalies in a series using the IQR method.\n",
    "    \n",
    "    Args:\n",
    "        series (pd.Series): Series to analyze.\n",
    "        log_transform (bool): Apply log transformation for skewed data.\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series: Boolean mask of anomalies.\n",
    "    \"\"\"\n",
    "    if log_transform:\n",
    "        series = np.log1p(series)\n",
    "    Q1, Q3 = series.quantile(0.25), series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound, upper_bound = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
    "    return (series < lower_bound) | (series > upper_bound)\n",
    "\n",
    "\n",
    "def detect_anomalies_zscore_series(\n",
    "        series: pd.Series, \n",
    "        log_transform: bool = False\n",
    ") -> pd.Series:\n",
    "    \"\"\"Detect anomalies in a series using the z-score method.\n",
    "    \n",
    "    Args:\n",
    "        series (pd.Series): Series to analyze.\n",
    "        log_transform (bool): Apply log transformation for skewed data.\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series: Boolean mask of anomalies.\n",
    "    \"\"\"\n",
    "    if log_transform:\n",
    "        series = np.log1p(series)\n",
    "    mean = series.mean()\n",
    "    std = series.std()\n",
    "    return (series - mean).abs() > 3 * std\n",
    "\n",
    "\n",
    "def visualize_data(\n",
    "        df: pd.DataFrame, \n",
    "        stock_symbol: str, \n",
    "        fetch_id: str\n",
    ") -> None:\n",
    "    \"\"\"Visualize key stock data columns.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Stock data DataFrame.\n",
    "        stock_symbol (str): Stock symbol for plot titles.\n",
    "        fetch_id (str): Unique identifier for the data fetch.\n",
    "    \"\"\"\n",
    "    if df.empty or not all(col in df for col in ['close', 'volume']):\n",
    "        raise ValueError(\"DataFrame is empty or lacks required columns: ('close', 'volume')\")\n",
    "    \n",
    "    # Plot closing price\n",
    "    plot_time_series(df, stock_symbol, fetch_id, 'close', save=True)\n",
    "    # Plot trading volume\n",
    "    plot_time_series(df, stock_symbol, fetch_id, 'volume', save=True)\n",
    "\n",
    "\n",
    "def save_outliers(\n",
    "    outliers_list: List[Dict], \n",
    "    stock_symbol: str, \n",
    "    fetch_id: str, \n",
    "    potential: bool = False\n",
    ") -> None:\n",
    "    \"\"\"Save confirmed outliers to JSON file.\"\"\"\n",
    "    outlier_data = {\n",
    "        \"fetch_id\": fetch_id,\n",
    "        \"stock_symbol\": stock_symbol,\n",
    "        \"outliers\": outliers_list\n",
    "    }\n",
    "    \n",
    "    outliers_dir = os.path.join(PROJECT_ROOT, \"data\", \"outliers\")\n",
    "    os.makedirs(outliers_dir, exist_ok=True)\n",
    "\n",
    "    file_name = \"potential_outliers.json\" if potential else \"outliers.json\"\n",
    "    history_file_name = \"potential_outliers_history.jsonl\" if potential else \"outliers_history.jsonl\"\n",
    "    file_path = os.path.join(outliers_dir, file_name)\n",
    "    history_file_path = os.path.join(outliers_dir, history_file_name)\n",
    "\n",
    "    logger.info(f\"Attempting to save outliers to: {os.path.abspath(file_path)}\")\n",
    "    try:\n",
    "        with open(file_path, \"w\") as f:\n",
    "            json.dump(outlier_data, f, indent=4)\n",
    "        logger.info(f\"Successfully wrote to {os.path.abspath(file_path)}\")\n",
    "        with open(history_file_path, \"a\") as f:\n",
    "            f.write(json.dumps(outlier_data) + \"\\n\")\n",
    "        logger.info(f\"Appended to {os.path.abspath(history_file_path)}\")\n",
    "    except (OSError, TypeError) as e:\n",
    "        logger.error(f\"Error saving outliers to {file_path}: {str(e)}\", extra={\"fetch_id\": fetch_id})\n",
    "        raise\n",
    "\n",
    "\n",
    "def generate_summary(\n",
    "    df: pd.DataFrame,\n",
    "    anomalies_df: Optional[pd.DataFrame] = None\n",
    ") -> None:\n",
    "    \"\"\"Generate and print a formatted summary of the DataFrame, including anomalies and statistics.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Stock data DataFrame with a DatetimeIndex.\n",
    "        anomalies_df (Optional[pd.DataFrame]): DataFrame containing detected anomalies, if any.\n",
    "\n",
    "    Returns:\n",
    "        None: Prints the summary directly using Markdown in Jupyter.\n",
    "    \"\"\"\n",
    "    # Print Data Summary\n",
    "    lines = [\n",
    "        \"#### **Data**\",\n",
    "        f\"- **Row Count**: {len(df)}\",\n",
    "        f\"- **Date Range**: {df.index.min().date()} to {df.index.max().date()}\",\n",
    "        f\"- **Missing Values**: {df.isna().sum().sum()}\",\n",
    "        f\"- **Anomalies**: {len(anomalies_df) if anomalies_df is not None else 'Not provided'}\"\n",
    "    ]\n",
    "    display(Markdown('\\n'.join(lines)))\n",
    "\n",
    "    # Print Anomalies Details if provided and not empty\n",
    "    if anomalies_df is not None and not anomalies_df.empty:\n",
    "        print(\"\\n\")\n",
    "        display(Markdown(\"#### **Anomalies**\"))\n",
    "        print(format_df(anomalies_df).to_string(index=True))\n",
    "\n",
    "    # Print Summary Statistics\n",
    "    stats_df = df.describe()\n",
    "    if 'volume' in stats_df.columns:\n",
    "        stats_df['volume'] = stats_df['volume'].round().astype(int)\n",
    "    print(\"\\n\")\n",
    "    display(Markdown(\"#### **Statistics**\"))\n",
    "    print(format_df(stats_df).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c248ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    stock_symbol: str, \n",
    "    fetch_id: str, \n",
    "    config_path: str, \n",
    "    logger\n",
    ") -> None:\n",
    "    \"\"\"Orchestrate the stock data inspection process, including loading, verifying, and analyzing data.\n",
    "\n",
    "    This function coordinates the loading of configuration and data, verification of data integrity,\n",
    "    detection of gaps and anomalies, visualization, and saving of results.\n",
    "\n",
    "    Args:\n",
    "        stock_symbol (str): Stock symbol for the data (e.g., 'TSLA').\n",
    "        fetch_id (str): Unique identifier for the data fetch.\n",
    "        config_path (str): Path to the configuration file.\n",
    "        logger: Logger instance for logging messages.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Raises:\n",
    "        SystemExit: If an error occurs during execution, logs the error and exits with status code 1.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        config = load_config(config_path=config_path, logger=logger)\n",
    "        logger.info(f\"Using stock symbol: {stock_symbol} and fetch_id: {fetch_id}\", extra={\"fetch_id\": fetch_id})\n",
    "        \n",
    "        # Load and process data\n",
    "        df = load_data(config, stock_symbol, fetch_id, logger)\n",
    "        verify_data(df, fetch_id)\n",
    "        gap_results = verify_gaps(df, fetch_id)\n",
    "        export_gaps(gap_results, stock_symbol, fetch_id)\n",
    "        anomalies = detect_anomalies(df, fetch_id, detection_method=detect_anomalies_iqr_series)\n",
    "        visualize_data(df, stock_symbol, fetch_id)\n",
    "        save_outliers(anomalies.to_dict(orient='records'), stock_symbol, fetch_id)\n",
    "        generate_summary(df, fetch_id, anomalies)\n",
    "        \n",
    "        logger.info(\"--------\", extra={\"fetch_id\": fetch_id})\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in execution: {str(e)}\", extra={\"fetch_id\": fetch_id if 'fetch_id' in locals() else \"N/A\"})\n",
    "        raise SystemExit(1)\n",
    "\n",
    "# Comment out automatic execution in notebook\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43b15c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config\n",
    "config = load_config(logger=logger)\n",
    "\n",
    "# Get stock_symbol and fetch ID, using Papermill parameters if provided\n",
    "stock_symbol = stock_symbol if stock_symbol is not None else get_stock_symbol(config=config)\n",
    "fetch_id = fetch_id if fetch_id is not None else get_fetch_id(config=config)\n",
    "config_path = config_path\n",
    "\n",
    "logger.info(f\"Using stock symbol: {stock_symbol} and fetch_id: {fetch_id}\")\n",
    "\n",
    "# main(stock_symbol, fetch_id, config_path, logger) # For papermill execution\n",
    "\n",
    "# Load and display data\n",
    "df = load_data(config, stock_symbol, fetch_id, logger=logger)\n",
    "print(df.shape)\n",
    "display(format_df(df.head()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a869f6c",
   "metadata": {},
   "source": [
    "## Data Verification\n",
    "\n",
    "This section checks the dataset for completeness and correctness, including row counts, date ranges, gaps, missing values, data types, negative values, and basic statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fe66a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaps = verify_gaps(df, fetch_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a61fd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_gaps(gaps, stock_symbol, fetch_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b00c76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_data(df, fetch_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15cba39",
   "metadata": {},
   "source": [
    "## Anomaly Detection\n",
    "\n",
    "Here, we identify potential anomalies in the data using statistical methods like the Interquartile Range (IQR), with special consideration for skewed data like volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1164c077",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies_df = detect_anomalies(df, fetch_id, detection_method=detect_anomalies_iqr_series)\n",
    "logger.info(\"Detected Anomalies:\")\n",
    "logger.info(f\"Anomalies detected:\\n{format_df(anomalies_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad34c33d",
   "metadata": {},
   "source": [
    "## Outlier Analysis\n",
    "\n",
    "This section examines specific anomalies to confirm whether they are genuine outliers, using statistical measures and contextual information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04037d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List potential outliers\n",
    "potential_outliers = anomalies_df.reset_index().to_dict(orient='records')\n",
    "print(\"Potential outliers for manual review:\")\n",
    "for outlier in potential_outliers:\n",
    "    print(outlier)\n",
    "\n",
    "# Save potential outliers\n",
    "save_outliers(potential_outliers, stock_symbol, fetch_id, potential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f717fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Manual review of outliers\n",
    "\n",
    "# Analyze volume on 2025-06-05\n",
    "print(\"Surrounding Data (2025-06-04 to 2025-06-06):\")\n",
    "display(format_df(df.loc['2025-06-04':'2025-06-06']))\n",
    "\n",
    "# Z-score for volume on 2025-06-05\n",
    "volume_on_date = df.loc['2025-06-05', 'volume']\n",
    "z_score = (volume_on_date - df['volume'].mean()) / df['volume'].std()\n",
    "logger.info(f\"Z-score for volume on 2025-06-05: {z_score}\")\n",
    "\n",
    "# IQR validation for volume\n",
    "Q1, Q3 = df['volume'].quantile(0.25), df['volume'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "print(f\"IQR upper bound for volume: {upper_bound}\")\n",
    "print(f\"Is 292,818,655 an outlier? {292818655 > upper_bound}\")\n",
    "\n",
    "# Contextual information\n",
    "print(\"Event Context: US President Donald Trump threatened to pull government contracts from Elon Musk's companies, impacting the stock.\")\n",
    "\n",
    "# Define NOT real outliers\n",
    "not_real_outliers = [\n",
    "    # {\"date\": \"2025-06-05\", \"column\": \"volume\", \"value\": \"1000\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b926c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Automate outlier confirmation in script version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150ac44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine confirmed_outliers based on auto_confirm_outliers or semi-automatic filtering\n",
    "logger.info(f\"Processing outliers with auto_confirm_outliers={auto_confirm_outliers}\", extra={\"fetch_id\": fetch_id})\n",
    "\n",
    "try:\n",
    "    if auto_confirm_outliers:\n",
    "        logger.info(\"Auto-confirming all potential outliers.\", extra={\"fetch_id\": fetch_id})\n",
    "        confirmed_outliers = potential_outliers\n",
    "    else:\n",
    "        try:\n",
    "            not_real_outliers_set = {(item['date'], item['column'], item['value']) for item in not_real_outliers}\n",
    "            logger.info(f\"Non-real outliers defined: {not_real_outliers_set}\", extra={\"fetch_id\": fetch_id})\n",
    "        except NameError:\n",
    "            not_real_outliers_set = set()\n",
    "            logger.warning(\"not_real_outliers not defined, treating all potential outliers as confirmed\", extra={\"fetch_id\": fetch_id})\n",
    "        confirmed_outliers = [\n",
    "            outlier for outlier in potential_outliers\n",
    "            if (outlier['date'], outlier['column'], outlier['value']) not in not_real_outliers_set\n",
    "        ]\n",
    "\n",
    "    # Add summary log with the number of confirmed outliers for better traceability\n",
    "    logger.info(f\"Number of confirmed outliers: {len(confirmed_outliers)}\", extra={\"fetch_id\": fetch_id})\n",
    "\n",
    "    # Save the confirmed outliers\n",
    "    save_outliers(confirmed_outliers, stock_symbol, fetch_id)\n",
    "\n",
    "    # Log each confirmed outlier for detailed traceability\n",
    "    logger.info(\"Confirmed outliers saved:\", extra={\"fetch_id\": fetch_id})\n",
    "    for outlier in confirmed_outliers:\n",
    "        logger.info(f\"Date: {outlier['date']}, Column: {outlier['column']}, Value: {outlier['value']}\", extra={\"fetch_id\": fetch_id})\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error processing outliers: {str(e)}\", extra={\"fetch_id\": fetch_id})\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fd7e09",
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "source": [
    "## Visualization\n",
    "\n",
    "Visualizations of closing price and trading volume help identify trends and anomalies visually, complementing the statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998dcb34",
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "visualize_data(df, stock_symbol, fetch_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd7e0bb",
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "source": [
    "## Quantitative Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de27748",
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "generate_summary(df, anomalies_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7f569b",
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "print(\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bbe1d0",
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "source": [
    "## Comprehensive Analysis and Insights\n",
    "\n",
    "### Data Coverage\n",
    "- **Row Count**: 250 trading days (June 17, 2024 - June 16, 2025).  \n",
    "- **Date Range**: 364 days total, with 250 trading days per U.S. market (e.g., NASDAQ) conventions.\n",
    "\n",
    "### Trading Day Observations\n",
    "Markets closed Jan. 9, 2025, for Carter’s mourning; open Oct. 14, 2024 (Columbus Day) and Nov. 11, 2024 (Veterans Day), aligning with U.S. equity calendars for data integrity.\n",
    "\n",
    "- **Data Completeness and TypesMissing Values**: None.  \n",
    "- **Data Types**: Numerical columns as float64; index as datetime64.\n",
    "\n",
    "### Anomalies Detected\n",
    "- **Volume Outlier**: June 5, 2025, with 292,818,655 shares (mean: 100,814,958; std: 39,521,561).   \n",
    "    - **Validation**: Z-score 4.858 (>4.8 std), exceeds IQR upper bound; matches NASDAQ data (Yahoo Finance ~1.8% lower, likely adjusted).  \n",
    "    - **Event Context**: Triggered by Trump’s threat to cancel Musk contracts, causing a 14.2% price drop ($332.05 to $284.70).  \n",
    "    - **Action**: Flagged is_outlier = True, retained for modeling.\n",
    "\n",
    "### Visualization Insights\n",
    "- **Closing Price**: Rose from $181.57 (mid-2024) to $479.86 (early 2025), fell to $300–$350 by June 2025; 14.2% drop on June 5 with partial recovery.  \n",
    "- **Trading Volume**: Right-skewed (median: 93.9M, mean: 100.8M); spiked to 292.8M on June 5 vs. 50–150M baseline, indicating event-driven volatility.\n",
    "\n",
    "### Conclusion\n",
    "Dataset is robust with 250 trading days, no missing values, and market calendar alignment. A notable volume outlier on June 5, 2025 (292,818,655 shares), validated by z-score (4.858) and linked to a 14.2% price drop, is retained for modeling. Visualizations reveal volatility trends and event sensitivity, with recovery patterns post-spike."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10967c6b",
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "source": [
    "<style>\n",
    ":root {\n",
    "    --jp-rendermime-error-background: white;\n",
    "}\n",
    "</style>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Stock Prediction Env",
   "language": "python",
   "name": "stock-prediction"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
